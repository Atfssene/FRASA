{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization Model - FRASA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1rOeaQy-nSG8pOejCG-i9iGk8pTsdRpSM",
      "authorship_tag": "ABX9TyOvyAHgzkOAHEqp7hSpwXXT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atfssene/FRASA/blob/main/Text_Summarization_Model_FRASA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUbyYSyrmBZ"
      },
      "source": [
        "# Text Summarization Model\n",
        "\n",
        "In this notebook, we will create a model for text summarization task. TextRank and SumBasic will be our feature extraction from senteces to create a weights that will be feeded to a neural networks. Let's start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_nXmMwCsg0U"
      },
      "source": [
        "## Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaRtAyxRmbcE",
        "outputId": "34c9601a-5be5-4e1b-d537-aece479b75ac"
      },
      "source": [
        "!pip install Sastrawi\n",
        "# !pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Sastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 17.0MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40kB 23.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51kB 26.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61kB 27.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81kB 24.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 22.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 174kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 184kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 194kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 204kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 22.5MB/s \n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9EgIp0lm9A2",
        "outputId": "dc5f748e-8c57-454d-a4d7-938676fa0667"
      },
      "source": [
        "# Import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import networkx as nx\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# For pre trained text embedding from FastText\n",
        "# import gzip\n",
        "# import fasttext\n",
        "# import fasttext.util\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "stop_words = factory.get_stop_words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbNj-Rs1vBC8"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp_uze8hvDZp",
        "outputId": "e7564fee-0849-4578-81bc-af27f1abef37"
      },
      "source": [
        "train = tf.keras.utils.get_file('train.csv', 'https://raw.githubusercontent.com/Atfssene/FRASA/main/Text%20Summarization/train.csv')\n",
        "test = tf.keras.utils.get_file('test.csv', 'https://raw.githubusercontent.com/Atfssene/FRASA/main/Text%20Summarization/test.csv')\n",
        "\n",
        "df_train = pd.read_csv(train, dtype=object, converters={'labels':eval})\n",
        "df_test = pd.read_csv(test, dtype=object, converters={'labels':eval})\n",
        "df_train.info()\n",
        "# df_test.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/Atfssene/FRASA/main/Text%20Summarization/train.csv\n",
            "39698432/39693272 [==============================] - 0s 0us/step\n",
            "Downloading data from https://raw.githubusercontent.com/Atfssene/FRASA/main/Text%20Summarization/test.csv\n",
            "9879552/9872406 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: ParserWarning: Both a converter and dtype were specified for column labels - only the converter will be used\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15012 entries, 0 to 15011\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   labels      15012 non-null  object\n",
            " 1   paragraphs  15012 non-null  object\n",
            " 2   summary     15012 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 352.0+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: ParserWarning: Both a converter and dtype were specified for column labels - only the converter will be used\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11hAK4cQrqQl"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "_BN0G6dUhfnX",
        "outputId": "fbf4d63d-419d-401b-f7d7-28c415a8bca0"
      },
      "source": [
        "# take in row [label, paragraphs, summary] => use apply\n",
        "# for labels convert False/True to 0/1\n",
        "# for paragraphs and summary, clean the data, \n",
        "\n",
        "def preprocess(row):\n",
        "\n",
        "  sentences = []\n",
        "  processed = []\n",
        "  for row in sent_tokenize(row):\n",
        "    sentences.append(sent_tokenize(row.lower()))\n",
        "  sentences = [y for x in sentences for y in x]\n",
        "  for i, text in enumerate(sentences):\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\w{1,3}\\b\",\" \",text)\n",
        "    text = \" \".join([word for word in text.split() if not word in stop_words])\n",
        "    processed.append(text)\n",
        "  return processed\n",
        "\n",
        "df_train['coba'] = (df_train[:5].apply(lambda row: preprocess(row['paragraphs']), axis=1))\n",
        "df_train\n",
        "# for x in xy:\n",
        "  # print(len(x), x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>paragraphs</th>\n",
              "      <th>summary</th>\n",
              "      <th>coba</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[False, True, True, True, False, False, False,...</td>\n",
              "      <td>Jakarta, CNN Indonesia - - Dokter Ryan Thamrin...</td>\n",
              "      <td>Dokter Lula Kamal yang merupakan selebriti sek...</td>\n",
              "      <td>[jakarta indonesia dokter ryan thamrin terkena...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[False, False, False, False, False, True, True...</td>\n",
              "      <td>Selfie ialah salah satu tema terpanas di kalan...</td>\n",
              "      <td>Asus memperkenalkan   ZenFone generasi keempat...</td>\n",
              "      <td>[selfie ialah salah satu tema terpanas kalanga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[True, True, False, False, False, False, False...</td>\n",
              "      <td>Jakarta, CNN Indonesia - - Dinas Pariwisata Pr...</td>\n",
              "      <td>Dinas Pariwisata Provinsi Bengkulu kembali men...</td>\n",
              "      <td>[jakarta indonesia dinas pariwisata provinsi b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[True, True, False, False, False, True, False,...</td>\n",
              "      <td>Merdeka.com - Indonesia Corruption Watch (ICW)...</td>\n",
              "      <td>Indonesia Corruption Watch (ICW) meminta Komis...</td>\n",
              "      <td>[merdeka indonesia corruption watch meminta ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[False, True, True, True, True, False, False, ...</td>\n",
              "      <td>Merdeka.com - Presiden Joko Widodo (Jokowi) me...</td>\n",
              "      <td>Jokowi memimpin upacara penurunan bendera. Usa...</td>\n",
              "      <td>[merdeka presiden joko widodo jokowi memimpin ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15007</th>\n",
              "      <td>[True, True, False, False, True, True, False, ...</td>\n",
              "      <td>MANCHESTER, JUARA.net - Mantan striker Manches...</td>\n",
              "      <td>Mantan striker Manchester United, Andrew' Andy...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15008</th>\n",
              "      <td>[True, True, True, False, False, False, False,...</td>\n",
              "      <td>Jakarta, CNN Indonesia - - Ratu Tisha Destria ...</td>\n",
              "      <td>Ratu Tisha Destria terpilih menjadi Sekretaris...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15009</th>\n",
              "      <td>[True, True, True, True, False, False, False, ...</td>\n",
              "      <td>ITALIA - Borussia Dortmund berhasil lolos ke b...</td>\n",
              "      <td>Borussia Dortmund lolos ke babak 16 Liga Europ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15010</th>\n",
              "      <td>[True, False, True, False, False, False, False...</td>\n",
              "      <td>AC Milan kembali ke jalur kemenangan dengan me...</td>\n",
              "      <td>AC Milan kembali ke jalur kemenangan pasca dit...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15011</th>\n",
              "      <td>[True, True, False, True, False, False, False,...</td>\n",
              "      <td>Jakarta, CNN Indonesia - - Ketua DPP PDIP Hend...</td>\n",
              "      <td>Ketua DPP PDIP Hendrawan Supratikno menyatakan...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15012 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  labels  ...                                               coba\n",
              "0      [False, True, True, True, False, False, False,...  ...  [jakarta indonesia dokter ryan thamrin terkena...\n",
              "1      [False, False, False, False, False, True, True...  ...  [selfie ialah salah satu tema terpanas kalanga...\n",
              "2      [True, True, False, False, False, False, False...  ...  [jakarta indonesia dinas pariwisata provinsi b...\n",
              "3      [True, True, False, False, False, True, False,...  ...  [merdeka indonesia corruption watch meminta ko...\n",
              "4      [False, True, True, True, True, False, False, ...  ...  [merdeka presiden joko widodo jokowi memimpin ...\n",
              "...                                                  ...  ...                                                ...\n",
              "15007  [True, True, False, False, True, True, False, ...  ...                                                NaN\n",
              "15008  [True, True, True, False, False, False, False,...  ...                                                NaN\n",
              "15009  [True, True, True, True, False, False, False, ...  ...                                                NaN\n",
              "15010  [True, False, True, False, False, False, False...  ...                                                NaN\n",
              "15011  [True, True, False, True, False, False, False,...  ...                                                NaN\n",
              "\n",
              "[15012 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7emUrQTNKOzd"
      },
      "source": [
        "Pre-processing raw text for feature extraction with rules:\n",
        "1. Splits paragraphs into sentences.\n",
        "2. Lowercasing letter.\n",
        "3. Remove punctuation.\n",
        "4. Remove stopword.\n",
        "5. Remove non alphanumerical letter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOYhez7LISq"
      },
      "source": [
        "factory = StopWordRemoverFactory()\n",
        "stop_words = factory.get_stop_words()\n",
        "\n",
        "def preprocess_text(row):\n",
        "  sentences = []\n",
        "  processed = \"\"\n",
        "  for row in sent_tokenize(row['clean_paragraphs']):\n",
        "    sentences.append(sent_tokenize(row.lower()))\n",
        "  sentences = [y for x in sentences for y in x]\n",
        "  for i, text in enumerate(sentences):\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\w{1,3}\\b\",\" \",text)\n",
        "    text = \" \".join([word for word in text.split() if not word in stop_words])\n",
        "    processed = processed + text +\". \"\n",
        "  return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whdRwJzpLZs1"
      },
      "source": [
        "train['preprocess_text'] = train.apply(lambda row: preprocess(row), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0wybsfdi2_k"
      },
      "source": [
        "Convert gold labels into binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4GDGqaHeQOs"
      },
      "source": [
        "def convert_binary(label_row):\n",
        "  labels = []\n",
        "  for label in label_row:\n",
        "    if label == True:\n",
        "      labels.append(1)\n",
        "    elif label == False:\n",
        "      labels.append(0)\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vraho13nrvew"
      },
      "source": [
        "Cleaning dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2VVWp2Ksnqh"
      },
      "source": [
        "factory = StopWordRemoverFactory()\n",
        "stop_words = factory.get_stop_words()\n",
        "\n",
        "def preprocess_text(row):\n",
        "  sentences = []\n",
        "  processed = \"\"\n",
        "  for row in sent_tokenize(row['clean_paragraphs']):\n",
        "    sentences.append(sent_tokenize(row.lower()))\n",
        "  sentences = [y for x in sentences for y in x]\n",
        "  for i, text in enumerate(sentences):\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\w{1,3}\\b\",\" \",text)\n",
        "    text = \" \".join([word for word in text.split() if not word in stop_words])\n",
        "    processed = processed + text +\". \"\n",
        "  return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4oK1ax5ruh0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "LDemmkOPhkKW",
        "outputId": "b6b99ba9-3a8d-4d20-e885-b8df759aee4d"
      },
      "source": [
        "df_train['binary_label'] = df_train.apply(lambda row: convert_binary(row['labels']),axis=1)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>paragraphs</th>\n",
              "      <th>summary</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>binary_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[False, True, True, True, False, False, False,...</td>\n",
              "      <td>Jakarta, CNN Indonesia - - Dokter Ryan Thamrin...</td>\n",
              "      <td>Dokter Lula Kamal yang merupakan selebriti sek...</td>\n",
              "      <td>jakarta indonesia dokter ryan thamrin terkenal...</td>\n",
              "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[False, False, False, False, False, True, True...</td>\n",
              "      <td>Selfie ialah salah satu tema terpanas di kalan...</td>\n",
              "      <td>Asus memperkenalkan   ZenFone generasi keempat...</td>\n",
              "      <td>selfie ialah salah satu tema terpanas kalangan...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[True, True, False, False, False, False, False...</td>\n",
              "      <td>Jakarta, CNN Indonesia - - Dinas Pariwisata Pr...</td>\n",
              "      <td>Dinas Pariwisata Provinsi Bengkulu kembali men...</td>\n",
              "      <td>jakarta indonesia dinas pariwisata provinsi be...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[True, True, False, False, False, True, False,...</td>\n",
              "      <td>Merdeka.com - Indonesia Corruption Watch (ICW)...</td>\n",
              "      <td>Indonesia Corruption Watch (ICW) meminta Komis...</td>\n",
              "      <td>merdeka indonesia corruption watch meminta kom...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[False, True, True, True, True, False, False, ...</td>\n",
              "      <td>Merdeka.com - Presiden Joko Widodo (Jokowi) me...</td>\n",
              "      <td>Jokowi memimpin upacara penurunan bendera. Usa...</td>\n",
              "      <td>merdeka presiden joko widodo jokowi memimpin u...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              labels  ...                                       binary_label\n",
              "0  [False, True, True, True, False, False, False,...  ...  [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  [False, False, False, False, False, True, True...  ...  [0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
              "2  [True, True, False, False, False, False, False...  ...  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3  [True, True, False, False, False, True, False,...  ...                  [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
              "4  [False, True, True, True, True, False, False, ...  ...            [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQcw4FfCv8UV"
      },
      "source": [
        "## Create TextRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0U4uGqfP2AX"
      },
      "source": [
        "Download Indonesian word vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Ac3r7cZNy5"
      },
      "source": [
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.vec.gz\n",
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1c-5WcZfDx-"
      },
      "source": [
        "Unzipping..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClrUoK8keMbJ"
      },
      "source": [
        "# !gunzip cc.id.300.vec.gz\n",
        "# !gunzip cc.id.300.bin.gz\n",
        "# from gensim.models.keyedvectors import KeyedVectors\n",
        "# kv = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/model_summarization/cc.id.300.vec', limit=400000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDluorS_9PBa"
      },
      "source": [
        "# kv.save_word2vec_format(\"/content/cc.id.vec\", binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSXuO9VPfziR"
      },
      "source": [
        "Load pretrained words embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmXywkenPb5_",
        "outputId": "ff2f2d91-7313-43d8-a345-0d84a364a1f7"
      },
      "source": [
        "word_embeddings = {}\n",
        "file = open('/content/drive/MyDrive/model_summarization/cc.id.vec', encoding='utf-8')\n",
        "for f in file:\n",
        "    values = f.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "file.close()\n",
        "\n",
        "len(word_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej4esB1eQ9Nk"
      },
      "source": [
        "# For sorting return list\n",
        "def sorting(e):\n",
        "  return e[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgvVRcDBXvWO"
      },
      "source": [
        "TextRank Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtlJXhxeBCKO"
      },
      "source": [
        "def textrank(df):\n",
        "    sentences = sent_tokenize(df['paragraphs'])\n",
        "    clean_sentences = sent_tokenize(df['clean_text'])\n",
        "\n",
        "    sentence_vectors = []\n",
        "    for i in clean_sentences:\n",
        "      if len(i) != 0:\n",
        "        v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "      else:\n",
        "        v = np.zeros((300,))\n",
        "      sentence_vectors.append(v)\n",
        "\n",
        "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "    res = len(sentence_vectors)\n",
        "    res2 = len(sentences)\n",
        "    a = a + 1\n",
        "    if (res != res2):\n",
        "      \n",
        "      print(a, res, res2)\n",
        "      \n",
        "    pass\n",
        "    # for i in range(len(sentences)):\n",
        "    #   for j in range(len(sentences)):\n",
        "    #     if i != j:\n",
        "    #       # pass\n",
        "    #       sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
        "\n",
        "\n",
        "    # nx_graph = nx.from_numpy_array(sim_mat)\n",
        "    # scores = nx.pagerank_numpy(nx_graph)\n",
        "\n",
        "    # ranked_sentences = sorted(([scores[i],i+1,s] for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    # text_rank = []\n",
        "    # for index, sentence in enumerate(ranked_sentences):\n",
        "    #   sentence.insert(1, index+1)\n",
        "    #   text_rank.append(sentence)\n",
        "\n",
        "    # # Return list(TextRank weights, TextRank order, sentence order, sentence) => text_rank\n",
        "    # text_rank = sorted(text_rank,key=sorting)\n",
        "\n",
        "    # TR_weight = []\n",
        "    # TR_order = []\n",
        "    # for i in range(len(text_rank)):\n",
        "    #   TR_weight.append(text_rank[i][0])\n",
        "    #   TR_order.append(text_rank[i][1])\n",
        "    # Just Return 2 list(TextRank weights, TextRank order)\n",
        "    # return TR_weight#, TR_order\n",
        "coba = df_train.apply(lambda row: textrank(row), axis=1)\n",
        "coba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObtL9OV4PbcF"
      },
      "source": [
        "df_train['text_rank'] = df_train.apply(lambda row: textrank(row), axis=1)\n",
        "# df_train['sum_basic'], df_train['sum_basic_order'] = df_train.apply(lambda row: sumbasic(row['paragraphs'], row['clean_text']), axis=1)\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLzLaQc4YHCa"
      },
      "source": [
        "Example result from variable text_rank:\n",
        "\n",
        "\n",
        "```\n",
        "[0.06091013588314788, 1, 3, 'Lula menuturkan, sakit itu membuat Ryan mesti vakum dari semua kegiatannya, termasuk menjadi pembawa acara Dokter Oz Indonesia.']\n",
        "[0.05986087469041391, 2, 2, 'Dokter Lula Kamal yang merupakan selebriti sekaligus rekan kerja Ryan menyebut kawannya itu sudah sakit sejak setahun yang lalu.']\n",
        "[0.05837850794448731, 3, 5, 'Setahu saya dia orangnya sehat, tapi tahun lalu saya dengar dia sakit.']\n",
        "[0.05811819645865672, 4, 14, '“ Saya tidak tahu, barangkali penyakit yang dulu sama yang sekarang berbeda, atau penyebab kematiannya beda dari penyakit sebelumnya.']\n",
        "[0.05776311916576284, 5, 13, 'Meski demikian, ia mendengar beberapa kabar yang menyebut bahwa penyebab Ryan meninggal adalah karena jatuh di kamar mandi.']\n",
        "[0.05773574513429258, 6, 8, 'Lula yang mengenal Ryan sejak sebelum aktif berkarier di televisi mengaku belum sempat membesuk Ryan lantaran lokasi yang jauh.']\n",
        "[0.05656949054199408, 7, 1, 'Jakarta, CNN Indonesia - - Dokter Ryan Thamrin, yang terkenal lewat acara Dokter Oz Indonesia, meninggal dunia pada Jumat (4 / 8) dini hari.']\n",
        "[0.05628137259134671, 8, 16, 'Ryan Thamrin terkenal sebagai dokter yang rutin membagikan tips dan informasi kesehatan lewat tayangan Dokter Oz Indonesia.']\n",
        "[0.05626494382459023, 9, 6, '( Karena) sakitnya, ia langsung pulang ke Pekanbaru, jadi kami yang mau jenguk juga susah.']\n",
        "[0.056198999719088594, 10, 7, 'Barangkali mau istirahat, ya betul juga, kalau di Jakarta susah isirahatnya, \" kata Lula kepada CNNIndonesia.com, Jumat (4 / 8).']\n",
        "[0.0559482929036589, 11, 11, 'Enggak tahu berat sekali apa bagaimana, \" tutur Ryan.']\n",
        "[0.05588348948999913, 12, 12, 'Walau sudah setahun menderita sakit, Lula tak mengetahui apa penyebab pasti kematian Dr Oz Indonesia itu.']\n",
        "[0.05466616578431332, 13, 10, 'Itu saya enggak tahu, belum sempat jenguk dan enggak selamanya bisa dijenguk juga.']\n",
        "[0.05358955066414319, 14, 9, 'Dia juga tak tahu penyakit apa yang diderita Ryan. \"']\n",
        "[0.05168620622168481, 15, 17, 'Ryan menempuh Pendidikan Dokter pada tahun 2002 di Fakultas Kedokteran Universitas Gadjah Mada.']\n",
        "[0.050898801974925634, 16, 4, 'Kondisi itu membuat Ryan harus kembali ke kampung halamannya di Pekanbaru, Riau untuk menjalani istirahat. \"']\n",
        "[0.05009205343710138, 17, 18, 'Dia kemudian melanjutkan pendidikan Klinis Kesehatan Reproduksi dan Penyakit Menular Seksual di Mahachulalongkornrajavidyalaya University, Bangkok, Thailand pada 2004.']\n",
        "[0.04915405357039277, 18, 15, 'Kita kan enggak bisa mengambil kesimpulan, \" kata Lula.']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX2iAyYHN6lt"
      },
      "source": [
        "## Create SumBasic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8KJOTGOm7AN"
      },
      "source": [
        "SumBasic Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0s6l_uXZlB_"
      },
      "source": [
        "frequency = {}\n",
        "processed =  df_train['clean_text'].iloc[0]\n",
        "for word in word_tokenize(processed):\n",
        "  if word.isalnum():\n",
        "    if word not in frequency.keys():\n",
        "      frequency[word]=1\n",
        "    else:\n",
        "      frequency[word]+=1\n",
        "max_fre = max(frequency.values())\n",
        "for word in frequency.keys():\n",
        "    frequency[word]=(frequency[word]/max_fre)\n",
        "    \n",
        "scores = {}\n",
        "for i, sentence in enumerate((sent_tokenize(processed))):\n",
        "  for word in word_tokenize(sentence):\n",
        "    if word in frequency.keys():\n",
        "        if i not in scores.keys():\n",
        "          scores[i] = frequency[word]\n",
        "        else:\n",
        "          scores[i] += frequency[word]\n",
        "ranked_sentences = sorted(([scores[i],i,s] for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "\n",
        "# Return list(SumBasic weights, SumBasic order, sentence order, sentence) => sum_bas\n",
        "sum_bas = []\n",
        "for index, sentence in enumerate(ranked_sentences):\n",
        "  sentence.insert(1, index+1)\n",
        "  sum_bas.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b6O0fwrOR1l",
        "outputId": "979efdd6-21d0-42d8-8c05-188ad92a6833"
      },
      "source": [
        "for sentence in sorted(sum_bas,key=sorting):\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.727272727272726, 1, 0, 'Jakarta, CNN Indonesia - - Dokter Ryan Thamrin, yang terkenal lewat acara Dokter Oz Indonesia, meninggal dunia pada Jumat (4 / 8) dini hari.']\n",
            "[3.909090909090908, 4, 1, 'Dokter Lula Kamal yang merupakan selebriti sekaligus rekan kerja Ryan menyebut kawannya itu sudah sakit sejak setahun yang lalu.']\n",
            "[4.09090909090909, 2, 2, 'Lula menuturkan, sakit itu membuat Ryan mesti vakum dari semua kegiatannya, termasuk menjadi pembawa acara Dokter Oz Indonesia.']\n",
            "[1.9999999999999998, 12, 3, 'Kondisi itu membuat Ryan harus kembali ke kampung halamannya di Pekanbaru, Riau untuk menjalani istirahat. \"']\n",
            "[1.0909090909090908, 17, 4, 'Setahu saya dia orangnya sehat, tapi tahun lalu saya dengar dia sakit.']\n",
            "[0.9090909090909092, 18, 5, '( Karena) sakitnya, ia langsung pulang ke Pekanbaru, jadi kami yang mau jenguk juga susah.']\n",
            "[2.0, 11, 6, 'Barangkali mau istirahat, ya betul juga, kalau di Jakarta susah isirahatnya, \" kata Lula kepada CNNIndonesia.com, Jumat (4 / 8).']\n",
            "[3.7272727272727266, 5, 7, 'Lula yang mengenal Ryan sejak sebelum aktif berkarier di televisi mengaku belum sempat membesuk Ryan lantaran lokasi yang jauh.']\n",
            "[1.8181818181818183, 13, 8, 'Dia juga tak tahu penyakit apa yang diderita Ryan. \"']\n",
            "[1.6363636363636362, 14, 9, 'Itu saya enggak tahu, belum sempat jenguk dan enggak selamanya bisa dijenguk juga.']\n",
            "[2.090909090909091, 9, 10, 'Enggak tahu berat sekali apa bagaimana, \" tutur Ryan.']\n",
            "[2.0909090909090904, 10, 11, 'Walau sudah setahun menderita sakit, Lula tak mengetahui apa penyebab pasti kematian Dr Oz Indonesia itu.']\n",
            "[2.2727272727272725, 7, 12, 'Meski demikian, ia mendengar beberapa kabar yang menyebut bahwa penyebab Ryan meninggal adalah karena jatuh di kamar mandi.']\n",
            "[2.181818181818181, 8, 13, '“ Saya tidak tahu, barangkali penyakit yang dulu sama yang sekarang berbeda, atau penyebab kematiannya beda dari penyakit sebelumnya.']\n",
            "[1.2727272727272727, 16, 14, 'Kita kan enggak bisa mengambil kesimpulan, \" kata Lula.']\n",
            "[3.9090909090909083, 3, 15, 'Ryan Thamrin terkenal sebagai dokter yang rutin membagikan tips dan informasi kesehatan lewat tayangan Dokter Oz Indonesia.']\n",
            "[2.545454545454545, 6, 16, 'Ryan menempuh Pendidikan Dokter pada tahun 2002 di Fakultas Kedokteran Universitas Gadjah Mada.']\n",
            "[1.636363636363636, 15, 17, 'Dia kemudian melanjutkan pendidikan Klinis Kesehatan Reproduksi dan Penyakit Menular Seksual di Mahachulalongkornrajavidyalaya University, Bangkok, Thailand pada 2004.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIgdItGBPm2T"
      },
      "source": [
        "def count_both(rows):\n",
        "  for index, row in enumerate(rows['paragraphs']):\n",
        "    rows['text_rank'] = index\n",
        "\n",
        "  # Calling textrank, return with weight and textrank order\n",
        "  # Calling sumbasic, return with weight and sumbasic order\n",
        "  # create new column for dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvatAuS-UhFY",
        "outputId": "6170456c-4aca-49f2-9ab6-333259d0bef3"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15012 entries, 0 to 15011\n",
            "Data columns (total 6 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   labels      15012 non-null  object\n",
            " 1   paragraphs  15012 non-null  object\n",
            " 2   summary     15012 non-null  object\n",
            " 3   clean_text  15012 non-null  object\n",
            " 4   bin_labels  15012 non-null  object\n",
            " 5   text_rank   15012 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 703.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_1DUvkaPZKU"
      },
      "source": [
        "Run both for all rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbNDqOK86Kjg"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O984YV49ZZU9"
      },
      "source": [
        "df_train['bin_labels'] = df_train['labels'].apply(convert_binary)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}